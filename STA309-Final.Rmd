---
title: "STA 309 Final"
author: "James Kirchenwitz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
library(tidyverse)
library(caret)
library(printr)
library(mplot)
library(nnet)  
```

# Part One:

```{r}
# Reading in data
data <- read.csv('diabetes_data.csv')
data <- data %>%
  mutate(gender = as.factor(gender),
         smoking_history = as.factor(smoking_history),
         diabetes = as.factor(diabetes))
#glimpse(data)

# Training the data
dataIndex <- createDataPartition(1:nrow(diamonds), p = 0.8)$Resample1
data_train <- filter(data, row_number() %in% dataIndex)
data_test <- filter(data, !row_number() %in% dataIndex)

ctrl = trainControl(method = 'repeatedcv', number = 5, repeats = 10)


# Deciding what variable are necassary for a best fit


```

## Fitting Three Models
```{r}
# First is a logistic regression model with all predictors (requirment one)
set.seed(401)
mod.glm <- train(diabetes ~ blood_glucose_level + HbA1c_level + bmi + smoking_history + 
                heart_disease + hypertension + age + gender, 
                data = data_train,
                method="glm",
                family='binomial',
                trControl=ctrl)
# Deciding what variable are necassary for a best fit
summary(mod.glm)
```

## Creating other 2 models off of significant predictors
```{r, message=FALSE, results='hide'}

# Random forest model (Requirement 2)
set.seed(401)
mod.rf <- train(diabetes ~ blood_glucose_level + HbA1c_level + bmi + smoking_history + 
                heart_disease + hypertension + age + gender,
                data = data_train,
                method="rf",
                trControl=ctrl,
                tuneGrid=expand.grid(mtry=1:4),
                importance=TRUE)

# Third other method (Requirement 3, reduced model)
set.seed(401)
mod.nn <- train(diabetes ~ blood_glucose_level + HbA1c_level + bmi + age,
                data=data_train,
                method="nnet",
                tuneGrid=expand.grid(size=3,
                                     decay=c(10,100,1000)),
                trControl=ctrl,
                linout = FALSE)

```

```{r}
varImp(mod.rf$finalModel)
```
### Significant Predictors:
**I utilized the logistic regression model to pick out significant predictors. NOTE: I know there are many better ways like backwords selection, forwards selection, etc., but for simplicity on this final I did not mess around with that. Anyhow, it appears that from the logistic model blood glucose level, HbA1c level, BMI, and age were all significant predictors for predicting diabetes. Additionally I wanted to see what the random forest model outputted as significant. Hemoglobin levels, blood glucose levels, and age are the three most important predictors.**

### Models Predicted:
**I decided to utilize a logistic regression model, a neural network model, and a random forest model. The reasoning behind this is that with a logistic model the computation time is short, and the interpretation of significance is easy to find. The logistic model I used here helped distinguished the significant predictors listed above. The reasoning for a random forest is that I wanted to run a more advanced / complex prediction model. The same rational is present for the neural network, as the neural network along with random forest are more complex.**

# Part Two:
```{r}
set.seed(401)
a <- resamples(list(LogMod=mod.glm,
               NN=mod.nn,
               RandomForest=mod.rf))
set.seed(401)
summary(resamples(list(LogMod=mod.glm,
                       NN=mod.nn,
                       RandomForest=mod.rf)))
bwplot(a, metric = "Accuracy") 

```

```{r}
# Comparing Models
set.seed(401)
df <- data_test %>%
  mutate(pred.mod.rf = predict(mod.rf, newdata=data_test),
         pred.mod.nn = predict(mod.nn, newdata=data_test),
         pred.mod.glm = predict(mod.glm, newdata=data_test))


#glimpse(df)

# Do this with all data

# Matrix for Random Forest Accuracy
rf.pred.table <- table(df$pred.mod.rf, df$diabetes)
confusionMatrix(rf.pred.table)

# Matrix of Neural Network
nn.pred.table <- table(df$pred.mod.nn, df$diabetes)
confusionMatrix(nn.pred.table)

# Matrix for Logistic regression
log.pred.table <- table(df$pred.mod.glm, df$diabetes)
confusionMatrix(log.pred.table)

```


**NOTE: the accuracies keep changing with each iteration of the script, but order of importance stays**

**When deciding when looking at what model is going to be best at predicting diabetes, I am going to go with the random forest model as it exhibited the highest predictive model accuracy relative to the logistic and neural network models. In addition to the graph I created the confusion matrix to compare the accuracy in another method. Again, the random forest was the most accurate being 90.22% accurate. Followed by the logistic model which was 89.13% accurate. Lastly the neural network was only 74.46% accurate. I think that some issues with using the Random Forest is just computational time. I really cannot think of another predictive model that could be better but I am quite confident one exists.**